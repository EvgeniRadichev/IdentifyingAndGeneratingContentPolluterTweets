{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation with RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs72K_30foM5"
      },
      "source": [
        "## Tensorflow RNN text generator network (baseline)\n",
        "Based on the following text-generation tutorial from Tensorflow (https://www.tensorflow.org/tutorials/text/text_generation) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm29Bv4FZqxV"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLmS3ogbZvpG"
      },
      "source": [
        "### Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIT5HwSMjAAV"
      },
      "source": [
        "import pandas as pd\n",
        "legitimate_tweets_df = pd.read_csv(\"/content/drive/Shareddrives/CSCI 5523 Group Project/Data/legitimate_users_tweets_filtered.csv\",header=0)\n",
        "tweets = legitimate_tweets_df['tweet']\n",
        "tweets = tweets[tweets.notna()]\n",
        "#tweets.to_string(\"/content/drive/Shareddrives/CSCI 5523 Group Project/Data/GAN data/legitimate_users_tweets_only.txt\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "HhrnRQXami2w",
        "outputId": "e34f9d25-e39d-45dd-b981-2f2f77ab66c0"
      },
      "source": [
        "legitimate_tweets_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>created_at</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>614</td>\n",
              "      <td>5873834688</td>\n",
              "      <td>I wish I had more free time. I'd LOVE to see you!</td>\n",
              "      <td>2009-11-19 18:16:40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>614</td>\n",
              "      <td>5873809295</td>\n",
              "      <td>Tonight, tomorrow. On the plane at 5 pm.</td>\n",
              "      <td>2009-11-19 18:15:42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>614</td>\n",
              "      <td>5291252160</td>\n",
              "      <td>I'm at Carlucci's in Salt Lake City, UT http:/...</td>\n",
              "      <td>2009-10-30 11:24:52</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>614</td>\n",
              "      <td>5205651441</td>\n",
              "      <td>@spam @JannetteDavid</td>\n",
              "      <td>2009-10-27 12:17:35</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1038</td>\n",
              "      <td>5762418891</td>\n",
              "      <td>@dialupkid Mijn vriendin en ik hebben een geza...</td>\n",
              "      <td>2009-11-16 05:08:29</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2410894</th>\n",
              "      <td>93390990</td>\n",
              "      <td>6169834217</td>\n",
              "      <td>Earned the Tweeter Achievement! #epicpetwars h...</td>\n",
              "      <td>2009-11-29 07:39:54</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2410895</th>\n",
              "      <td>93390990</td>\n",
              "      <td>6168916131</td>\n",
              "      <td>meet me on ELIMINATE pro!!</td>\n",
              "      <td>2009-11-29 06:44:03</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2410896</th>\n",
              "      <td>93402679</td>\n",
              "      <td>6170059145</td>\n",
              "      <td>my twitter</td>\n",
              "      <td>2009-11-29 07:52:22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2410897</th>\n",
              "      <td>93419256</td>\n",
              "      <td>6171947104</td>\n",
              "      <td>exploring this thing...</td>\n",
              "      <td>2009-11-29 09:25:53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2410898</th>\n",
              "      <td>93426370</td>\n",
              "      <td>6172856187</td>\n",
              "      <td>learning the rap ice ice baby LOL, not very gd...</td>\n",
              "      <td>2009-11-29 10:08:09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2410899 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          user_id    tweet_id  ...           created_at label\n",
              "0             614  5873834688  ...  2009-11-19 18:16:40     0\n",
              "1             614  5873809295  ...  2009-11-19 18:15:42     0\n",
              "2             614  5291252160  ...  2009-10-30 11:24:52     0\n",
              "3             614  5205651441  ...  2009-10-27 12:17:35     0\n",
              "4            1038  5762418891  ...  2009-11-16 05:08:29     0\n",
              "...           ...         ...  ...                  ...   ...\n",
              "2410894  93390990  6169834217  ...  2009-11-29 07:39:54     0\n",
              "2410895  93390990  6168916131  ...  2009-11-29 06:44:03     0\n",
              "2410896  93402679  6170059145  ...  2009-11-29 07:52:22     0\n",
              "2410897  93419256  6171947104  ...  2009-11-29 09:25:53     0\n",
              "2410898  93426370  6172856187  ...  2009-11-29 10:08:09     0\n",
              "\n",
              "[2410899 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21wP-OuTjp8u"
      },
      "source": [
        "# sample 10% of tweets, remove NA entries\n",
        "legitimate_tweets_df_sample = legitimate_tweets_df.sample(legitimate_tweets_df.shape[0]//10)\n",
        "tweets_sample = legitimate_tweets_df_sample['tweet']\n",
        "tweets_sample = tweets_sample[tweets_sample.notna()]\n",
        "\n",
        "# save legitimate user tweets as text file\n",
        "path = \"/content/legitimate_users_tweets_only_sample.txt\"\n",
        "tweets_sample.to_string(path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMcHQM8WmIlj"
      },
      "source": [
        "# load text\n",
        "#text = open(path, 'rb').read().decode(encoding = 'utf-8')\n",
        "text = open(\"/content/drive/Shareddrives/CSCI 5523 Group Project/Data/GAN data/legitimate_users_tweets_only.txt\",'rb').read().decode(encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c923f44f-0674-46ee-c12b-fdb7edcccd37"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " I wish I had more free time. I'd LOVE to see you!\n",
            "          Tonight, tomorrow. On the plane at 5 pm.\n",
            " I'm at Carlucci's in Salt Lake City, UT http:/...\n",
            "                              @spam @JannetteDavid\n",
            " @dialupkid Mijn vriendin en ik hebben een gez\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a39d967-1bc8-4649-f73f-ca237d2d4600"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "## Text Vectorization\n",
        "\n",
        "Convert strings to a numerical representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a36ad67-ac9b-486a-ed07-70c74a2db92a"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `preprocessing.StringLookup(..., invert=True)`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "Use `tf.strings.reduce_join` to join the characters back into strings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d224da3-542f-489a-c1a3-47ebc0518db9"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd1e45d7-55f6-461b-b443-4d0ae9f0440f"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(122955848,), dtype=int64, numpy=array([ 3, 44,  3, ..., 17, 17, 17])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjH5v45-yqqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c03fa0f-4d6b-462d-f5d8-f6fd579bbf9a"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "I\n",
            " \n",
            "w\n",
            "i\n",
            "s\n",
            "h\n",
            " \n",
            "I\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656dc566-0aee-4529-946f-37a61ce7fb13"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b' ' b'I' b' ' b'w' b'i' b's' b'h' b' ' b'I' b' ' b'h' b'a' b'd' b' '\n",
            " b'm' b'o' b'r' b'e' b' ' b'f' b'r' b'e' b'e' b' ' b't' b'i' b'm' b'e'\n",
            " b'.' b' ' b'I' b\"'\" b'd' b' ' b'L' b'O' b'V' b'E' b' ' b't' b'o' b' '\n",
            " b's' b'e' b'e' b' ' b'y' b'o' b'u' b'!' b'\\n' b' ' b' ' b' ' b' ' b' '\n",
            " b' ' b' ' b' ' b' ' b' ' b'T' b'o' b'n' b'i' b'g' b'h' b't' b',' b' '\n",
            " b't' b'o' b'm' b'o' b'r' b'r' b'o' b'w' b'.' b' ' b'O' b'n' b' ' b't'\n",
            " b'h' b'e' b' ' b'p' b'l' b'a' b'n' b'e' b' ' b'a' b't' b' ' b'5' b' '\n",
            " b'p' b'm' b'.'], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO32cMWu4a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e560eccf-7329-4bb3-876f-1892a85d90ca"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\" I wish I had more free time. I'd LOVE to see you!\\n          Tonight, tomorrow. On the plane at 5 pm.\"\n",
            "b\"\\n I'm at Carlucci's in Salt Lake City, UT http:/...\\n                              @spam @JannetteDavi\"\n",
            "b'd\\n @dialupkid Mijn vriendin en ik hebben een geza...\\n Weet iemand wie de Amsterdamse screening van 1.'\n",
            "b'..\\n Gat in de markt: slow flying. RT @MarcDerby Wi...\\n @erwblo Het woord gratis doet hierbij wonderen'\n",
            "b'...\\n         Christmas Station http://flic.kr/p/7g5wYq\\n Vier vlaggen halfstok bij de politie op de Br'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8becbe4-2b96-4f63-ef8b-edbfd402bd22"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2e0eae-4e1c-46c9-e426-e56d54db3450"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b\" I wish I had more free time. I'd LOVE to see you!\\n          Tonight, tomorrow. On the plane at 5 pm\"\n",
            "Target: b\"I wish I had more free time. I'd LOVE to see you!\\n          Tonight, tomorrow. On the plane at 5 pm.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f223f6-ff8a-4f5b-f0ee-c74963e705d1"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((256, 100), (256, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f18896-af40-4d22-d63a-698fac79edf2"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 100, 99) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52035ad-4f9a-4cf6-812e-0eca8eac49ef"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      multiple                  25344     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  101475    \n",
            "=================================================================\n",
            "Total params: 4,065,123\n",
            "Trainable params: 4,065,123\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfe396f-ceb9-4370-f364-9c8ffd05a730"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([73, 58, 21, 94, 19, 52, 58, 98, 79, 56, 28,  2, 81, 57, 89,  1,  9,\n",
              "       71, 83, 37, 77, 63, 18,  8, 25, 63, 13,  1, 33, 65, 14, 66, 67,  4,\n",
              "       18, 92, 11, 73, 11, 97, 81, 51, 53, 88,  9, 71, 69, 55,  1, 28, 51,\n",
              "       79, 28, 44, 10, 49, 30, 27, 65, 39, 65, 33, 64, 97, 52, 74, 67, 64,\n",
              "       49, 41, 31, 75, 22, 30, 68, 75, 74, 37, 15, 30,  0, 24, 90, 87, 23,\n",
              "       71, 91, 35,  2, 13, 22, 52, 65, 63, 69, 49, 70, 49, 57,  3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563fc17f-5f0b-4b49-e658-5f1767a1a177"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'absoluto del mobile hoy en di...\\n @takag talvez mis genes son recesivos y mis hi...\\n @yoruguayo @rsi'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"fW2{0QW\\x7flU9\\nnVv[UNK]&dpBj\\\\/%6\\\\*[UNK]>^+_`!/y(f(~nPRu&dbT[UNK]9Pl9I'N;8^D^>]~Qg`]NF<h3;ahgB,;5wt4dx@\\n*3Q^\\\\bNcNV \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d499b45-8e15-4685-c70a-0a0b8db33ce8"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (256, 100, 99)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.59632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7096ec-3f45-48cd-d4d5-ceb2555ca93b"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.118904"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/content/drive/Shareddrives/CSCI 5523 Group Project/Models/Generative Model/training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1457bc-bae8-4a76-f06d-76be57788e09"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "4755/4755 [==============================] - 557s 117ms/step - loss: 1.9731\n",
            "Epoch 2/30\n",
            "4755/4755 [==============================] - 546s 115ms/step - loss: 1.4650\n",
            "Epoch 3/30\n",
            "4755/4755 [==============================] - 556s 117ms/step - loss: 1.4215\n",
            "Epoch 4/30\n",
            "4755/4755 [==============================] - 546s 115ms/step - loss: 1.4038\n",
            "Epoch 5/30\n",
            "4755/4755 [==============================] - 553s 116ms/step - loss: 1.3938\n",
            "Epoch 6/30\n",
            "4755/4755 [==============================] - 557s 117ms/step - loss: 1.3871\n",
            "Epoch 7/30\n",
            "4755/4755 [==============================] - 549s 115ms/step - loss: 1.3831\n",
            "Epoch 8/30\n",
            "4755/4755 [==============================] - 556s 117ms/step - loss: 1.3793\n",
            "Epoch 9/30\n",
            "4755/4755 [==============================] - 546s 115ms/step - loss: 1.3768\n",
            "Epoch 10/30\n",
            "4755/4755 [==============================] - 555s 117ms/step - loss: 1.3729\n",
            "Epoch 11/30\n",
            "4755/4755 [==============================] - 546s 115ms/step - loss: 1.3704\n",
            "Epoch 12/30\n",
            "4755/4755 [==============================] - 553s 116ms/step - loss: 1.3679\n",
            "Epoch 13/30\n",
            "4755/4755 [==============================] - 557s 117ms/step - loss: 1.3664\n",
            "Epoch 14/30\n",
            "4755/4755 [==============================] - 549s 115ms/step - loss: 1.3651\n",
            "Epoch 15/30\n",
            "4755/4755 [==============================] - 556s 117ms/step - loss: 1.3630\n",
            "Epoch 16/30\n",
            "4755/4755 [==============================] - 545s 114ms/step - loss: 1.3627\n",
            "Epoch 17/30\n",
            "4755/4755 [==============================] - 555s 117ms/step - loss: 1.3598\n",
            "Epoch 18/30\n",
            "4755/4755 [==============================] - 547s 115ms/step - loss: 1.3584\n",
            "Epoch 19/30\n",
            "4755/4755 [==============================] - 553s 116ms/step - loss: 1.3581\n",
            "Epoch 20/30\n",
            "4755/4755 [==============================] - 557s 117ms/step - loss: 1.3567\n",
            "Epoch 21/30\n",
            "4755/4755 [==============================] - 550s 115ms/step - loss: 1.3735\n",
            "Epoch 22/30\n",
            "4755/4755 [==============================] - 556s 117ms/step - loss: 1.3625\n",
            "Epoch 23/30\n",
            "4755/4755 [==============================] - 546s 115ms/step - loss: 1.3516\n",
            "Epoch 24/30\n",
            "4755/4755 [==============================] - 558s 117ms/step - loss: 1.3547\n",
            "Epoch 25/30\n",
            "4755/4755 [==============================] - 555s 116ms/step - loss: 1.3539\n",
            "Epoch 26/30\n",
            "4755/4755 [==============================] - 560s 118ms/step - loss: 1.3522\n",
            "Epoch 27/30\n",
            "4755/4755 [==============================] - 554s 116ms/step - loss: 1.3507\n",
            "Epoch 28/30\n",
            "4755/4755 [==============================] - 552s 116ms/step - loss: 1.3528\n",
            "Epoch 29/30\n",
            "4755/4755 [==============================] - 554s 116ms/step - loss: 1.3503\n",
            "Epoch 30/30\n",
            "4755/4755 [==============================] - 543s 114ms/step - loss: 1.3495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    print(predicted_logits)\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nibwwYa21bIL"
      },
      "source": [
        "out_path = \"/content/drive/Shareddrives/CSCI 5523 Group Project/Models/Generative Model/rnn_full_filtered_legitimate_tweets\"\n",
        "tf.saved_model.save(one_step_model, out_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run the code in a loop to generate some text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST7PSyk9t1mT"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['@'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(240):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n' + '_'*5)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYb5Gn5bSIIW"
      },
      "source": [
        "# extract string output from generated tensor\n",
        "l = [x.decode('utf-8') for x in result.numpy()]\n",
        "s = l[0].split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuTSouh6SvvT",
        "outputId": "b1a4b6a1-0878-49dd-9acf-d14c4de8a5ff"
      },
      "source": [
        "s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@paul_irasti Yessssss :P Nick :)',\n",
              " '                   I want Rachel Justin Bieber!!!!',\n",
              " \"         @GJ3312 At lunch its tyler I'll go to med\",\n",
              " '                              @PSBoneUSC naw it is',\n",
              " '                     @bieber_n1 argh awww =) enforce',\n",
              " ' @']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkLu7Y8UCMT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f7e7ec-2005-429e-b1d8-7379c9ac1ab4"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Taylor', 'Taylor', 'Taylor', 'Taylor', 'Taylor'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(5, 99), dtype=float32)\n",
            "Tensor(\"strided_slice:0\", shape=(5, 99), dtype=float32)\n",
            "tf.Tensor(\n",
            "[b'Taylor Swift!!! - Real Estate Recentmores To...\\n Review Of First Analyzing Volume I was dirty h...\\n Classi'\n",
            " b\"Taylor Lautner.\\n Ok normally my unilort because it's been nice ...\\n nothing thout of activats magarime n n\"\n",
            " b\"Taylor's :)\\n                       C'mon son. Ituff is so sma.\\n                                    OMG ste\"\n",
            " b\"Taylors.org:))))\\n @lanaessie great night! I'm freezing! They onl...\\n @iMakNornikki :'( :D Its not the day \"\n",
            " b'Taylor: Kellan Muscor: http...\\n get an emo magic video has a cute monkey....no...\\n       @aussie hello! :)'], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 0.7638649940490723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Restore Model and Generate Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Grk32H_CzsC"
      },
      "source": [
        "out_path = \"/content/drive/Shareddrives/CSCI 5523 Group Project/Models/Generative Model/rnn_full_filtered_legitimate_tweets\"\n",
        "#tf.saved_model.save(one_step_model, out_path)\n",
        "one_step_reloaded = tf.saved_model.load(out_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f321a7-599a-4b56-c069-492959dbb89a"
      },
      "source": [
        "states = None\n",
        "first_char = tf.constant(['T', 'R', 'I', 'S', 'A']) # top 5 characters tweets start with\n",
        "next_char = first_char\n",
        "result = [next_char]\n",
        "generated_tweets = []\n",
        "counter = 0\n",
        "\n",
        "# generate 1000 example tweets of length 140\n",
        "while counter < 200:\n",
        "  counter += 1\n",
        "  for n in range(140):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "  result = tf.strings.join(result)\n",
        "  new_tweets = [x.decode('utf-8') for x in result.numpy()]\n",
        "  generated_tweets = generated_tweets + new_tweets\n",
        "  result = [first_char]\n",
        "\n",
        "print(len(generated_tweets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpZDn8SlSPWY"
      },
      "source": [
        "# save generated tweets\n",
        "labels = np.ones((1000,)).astype(int)\n",
        "gen_tweets_df = pd.DataFrame({'generated tweets':generated_tweets,'label':labels})\n",
        "gen_tweets_df.to_csv('/content/drive/Shareddrives/CSCI 5523 Group Project/Data/GAN data/generated_tweets_1000.csv', header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuoFopbBT-ZC"
      },
      "source": [
        "# load generated tweets\n",
        "gen_tweets_df = pd.read_csv('/content/drive/Shareddrives/CSCI 5523 Group Project/Data/GAN data/generated_tweets_1000.csv', header = 0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo-ZVGQwcT3-"
      },
      "source": [
        "## Evaluate GAN Performance\n",
        "Using our best polluter tweet classifier, evaluate the performance of the text generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIeLNDr1rT1x",
        "outputId": "0f908aeb-dfa0-4e32-a34c-874543b95215"
      },
      "source": [
        "!pip install -q tensorflow-text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |                                | 10kB 4.1MB/s eta 0:00:01\r\u001b[K     |▏                               | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |▎                               | 30kB 7.3MB/s eta 0:00:01\r\u001b[K     |▍                               | 40kB 6.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 51kB 7.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 61kB 9.1MB/s eta 0:00:01\r\u001b[K     |▊                               | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |▉                               | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 92kB 9.6MB/s eta 0:00:01\r\u001b[K     |█                               | 102kB 9.8MB/s eta 0:00:01\r\u001b[K     |█                               | 112kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 122kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 133kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 143kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 153kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 163kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 174kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 184kB 9.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 194kB 9.8MB/s eta 0:00:01\r\u001b[K     |██                              | 204kB 9.8MB/s eta 0:00:01\r\u001b[K     |██                              | 215kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 225kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 235kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 245kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 256kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 266kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 276kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 286kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 296kB 9.8MB/s eta 0:00:01\r\u001b[K     |███                             | 307kB 9.8MB/s eta 0:00:01\r\u001b[K     |███                             | 317kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 327kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 337kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 348kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 358kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 368kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 378kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 389kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 399kB 9.8MB/s eta 0:00:01\r\u001b[K     |████                            | 409kB 9.8MB/s eta 0:00:01\r\u001b[K     |████                            | 419kB 9.8MB/s eta 0:00:01\r\u001b[K     |████                            | 430kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 440kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 450kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 460kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 471kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 481kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 491kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 501kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 512kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 522kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 532kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 542kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 552kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 563kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 573kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 583kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 593kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 604kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 614kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 624kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 634kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 645kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 655kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 665kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 675kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 686kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 696kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 706kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 716kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 727kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 737kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 747kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 757kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 768kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 778kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 788kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 798kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 808kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 819kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 829kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 839kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 849kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 860kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 870kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 880kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 890kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 901kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 911kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 921kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 931kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 942kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 952kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 962kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 972kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 983kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 993kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 1.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 1.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 1.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 1.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 1.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 1.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 1.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 1.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 1.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 1.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 2.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 2.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 2.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 2.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 2.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 2.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 2.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 2.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 2.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 2.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 2.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 2.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 2.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 2.5MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 2.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 2.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 2.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 2.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 2.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 2.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.6MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.7MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.8MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.9MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 3.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 3.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 3.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 3.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 3.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 3.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 3.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 3.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.0MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.1MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 3.2MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 3.3MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 3.4MB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 3.4MB 9.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mKDBWCVuJuR",
        "outputId": "956603c6-8ed2-417a-cb10-e3ed3574ef7c"
      },
      "source": [
        "!pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 12.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 56.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 53.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 136kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 63.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 706kB 43.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 50.1MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5S6uj80cSt0",
        "outputId": "7361bc57-7416-4cf8-a0e3-9ce32d161a49"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "from tensorflow_hub import KerasLayer\n",
        "import tensorflow_text as text\n",
        "classification_model = tf.keras.models.load_model('/content/drive/Shareddrives/CSCI 5523 Group Project/Models/bert.h5', custom_objects={'KerasLayer':KerasLayer})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpP1NtxXtQYO"
      },
      "source": [
        "from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "epochs = 5\n",
        "steps_per_epoch = 250\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = [tf.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "classification_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fqGu_-AtCm5",
        "outputId": "03df5e9e-ccd3-44bc-f826-8b370eed4ef6"
      },
      "source": [
        "classification_model.evaluate(x=gen_tweets_df['generated tweets'], y=gen_tweets_df['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 3s 66ms/step - loss: 2.7535 - binary_accuracy: 0.0053 - precision_1: 0.9091 - recall_1: 0.0053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.7102131843566895, 0.007000000216066837, 1.0, 0.007000000216066837]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOHrtxkJnZFQ"
      },
      "source": [
        "# Apply tokenization and vectorization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidfvec = TfidfVectorizer(ngram_range=(1,1), stop_words='english', max_features=2**13)\n",
        "\n",
        "# Fit the data and then return the matrix\n",
        "X = tfidfvec.fit_transform(gen_tweets_df['generated tweets'])\n",
        "\n",
        "# test set size of 20% of the data and the random seed 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), gen_tweets_df['label'], test_size=0.2, random_state=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFKBhkjXc9OZ",
        "outputId": "562685e3-abe3-4a34-ad16-4319fb212542"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "naive_bayes = MultinomialNB()\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "predictions = naive_bayes.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('Accuracy score: {}'.format(accuracy_score(y_test, predictions)))\n",
        "print('Precision score: {}'.format(precision_score(y_test, predictions)))\n",
        "print('Recall score: {}'.format(recall_score(y_test, predictions)))\n",
        "print('F1 score: {}'.format(f1_score(y_test, predictions)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 1.0\n",
            "Precision score: 1.0\n",
            "Recall score: 1.0\n",
            "F1 score: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}