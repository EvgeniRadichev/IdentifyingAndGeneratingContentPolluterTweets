{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANDREW's copy Classification - Extra.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB4q9PmBpGvX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwv8xIxjuG30",
        "outputId": "aadc0ffc-759d-4709-de54-9ffddf382bf5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6h6rR6D0efc"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "def print_metrics(y_test, predictions):\n",
        "  print('Accuracy score: {}'.format(accuracy_score(y_test, predictions)))\n",
        "  print('Precision score: {}'.format(precision_score(y_test, predictions)))\n",
        "  print('Recall score: {}'.format(recall_score(y_test, predictions)))\n",
        "  print('F1 score: {}'.format(f1_score(y_test, predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bwACEPO_MF2"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bSrGha8_LfH"
      },
      "source": [
        "pollutor_tweets_df = pd.read_csv(\"/content/drive/Shareddrives/CSCI 5523 Group Project/Data/content_polluters_tweets_filtered.csv\", header = 0)\n",
        "legitimate_users_df = pd.read_csv(\"/content/drive/Shareddrives/CSCI 5523 Group Project/Data/legitimate_users_tweets_filtered.csv\", header = 0)\n",
        "dataset = legitimate_users_df.append(pollutor_tweets_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MADrBkPBDzB"
      },
      "source": [
        "def write_df_to_dir(df, dir):\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    with open(dir+\"/\"+str(df.iloc[i][\"tweet_id\"])+\".txt\", \"w+\") as f:\n",
        "      f.write(df.iloc[i][\"tweet\"])\n",
        "      f.close()\n",
        "\n",
        "# write_df_to_dir(legitimate_users_df, \"legit\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBRPE-4Bpdla",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4abd18ab-521a-4f83-9a0b-77b3b32caed4"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>created_at</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>614</td>\n",
              "      <td>5873834688</td>\n",
              "      <td>I wish I had more free time. I'd LOVE to see you!</td>\n",
              "      <td>2009-11-19 18:16:40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>614</td>\n",
              "      <td>5873809295</td>\n",
              "      <td>Tonight, tomorrow. On the plane at 5 pm.</td>\n",
              "      <td>2009-11-19 18:15:42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>614</td>\n",
              "      <td>5291252160</td>\n",
              "      <td>I'm at Carlucci's in Salt Lake City, UT http:/...</td>\n",
              "      <td>2009-10-30 11:24:52</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>614</td>\n",
              "      <td>5205651441</td>\n",
              "      <td>@spam @JannetteDavid</td>\n",
              "      <td>2009-10-27 12:17:35</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1038</td>\n",
              "      <td>5762418891</td>\n",
              "      <td>@dialupkid Mijn vriendin en ik hebben een geza...</td>\n",
              "      <td>2009-11-16 05:08:29</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id    tweet_id  ...           created_at label\n",
              "0      614  5873834688  ...  2009-11-19 18:16:40     0\n",
              "1      614  5873809295  ...  2009-11-19 18:15:42     0\n",
              "2      614  5291252160  ...  2009-10-30 11:24:52     0\n",
              "3      614  5205651441  ...  2009-10-27 12:17:35     0\n",
              "4     1038  5762418891  ...  2009-11-16 05:08:29     0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHrPRFKN_VNg"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "011AQfzs-Um6"
      },
      "source": [
        "Remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzc9cW1X-Rf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "1bcb289b-5edd-4d87-dc64-377433c3e741"
      },
      "source": [
        "dataset.dropna(inplace=True)\n",
        "dataset = dataset[dataset.tweet != '']\n",
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4.560727e+06</td>\n",
              "      <td>4.560727e+06</td>\n",
              "      <td>4.560727e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.821501e+07</td>\n",
              "      <td>7.809418e+09</td>\n",
              "      <td>4.713784e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.829453e+07</td>\n",
              "      <td>3.871802e+09</td>\n",
              "      <td>4.991802e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.140000e+02</td>\n",
              "      <td>5.218033e+06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.474534e+07</td>\n",
              "      <td>5.585464e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.941459e+07</td>\n",
              "      <td>5.936497e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.434406e+07</td>\n",
              "      <td>9.117617e+09</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.737670e+08</td>\n",
              "      <td>2.014599e+10</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            user_id      tweet_id         label\n",
              "count  4.560727e+06  4.560727e+06  4.560727e+06\n",
              "mean   5.821501e+07  7.809418e+09  4.713784e-01\n",
              "std    3.829453e+07  3.871802e+09  4.991802e-01\n",
              "min    6.140000e+02  5.218033e+06  0.000000e+00\n",
              "25%    2.474534e+07  5.585464e+09  0.000000e+00\n",
              "50%    4.941459e+07  5.936497e+09  0.000000e+00\n",
              "75%    8.434406e+07  9.117617e+09  1.000000e+00\n",
              "max    1.737670e+08  2.014599e+10  1.000000e+00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7poy4gYAxi3"
      },
      "source": [
        "Drop null values, only consider 20,000 random tweets as a precaution for performance. Hughdan will adjust this later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEV6KtV9Az9k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "28722563-3c2f-4b40-a784-8627dd1bef0b"
      },
      "source": [
        "#dataset_sample = dataset.sample(frac=1, random_state=1).head(100000*5)\n",
        "dataset_sample = dataset.sample(frac=1, random_state=1)\n",
        "dataset_sample.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>created_at</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>842049</th>\n",
              "      <td>26056810</td>\n",
              "      <td>5693671561</td>\n",
              "      <td>Closet Doors (Eastmoreland Heights): Closet do...</td>\n",
              "      <td>2009-11-13 16:54:34</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1503267</th>\n",
              "      <td>103892446</td>\n",
              "      <td>9516790689</td>\n",
              "      <td>Tessa Virtue, Scott Moir win Canada's first ic...</td>\n",
              "      <td>2010-02-23 01:44:15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246729</th>\n",
              "      <td>37526413</td>\n",
              "      <td>5914025144</td>\n",
              "      <td>Build failure for libarchive (u=0, c=0, b=50, ...</td>\n",
              "      <td>2009-11-21 01:51:01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1925994</th>\n",
              "      <td>63472113</td>\n",
              "      <td>5759543363</td>\n",
              "      <td>msh ngantukkk</td>\n",
              "      <td>2009-11-16 01:21:12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536452</th>\n",
              "      <td>38032306</td>\n",
              "      <td>13863317186</td>\n",
              "      <td>New Video-Mentions San Diego-Today on Good Mor...</td>\n",
              "      <td>2010-05-12 12:46:42</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           user_id     tweet_id  ...           created_at label\n",
              "842049    26056810   5693671561  ...  2009-11-13 16:54:34     0\n",
              "1503267  103892446   9516790689  ...  2010-02-23 01:44:15     1\n",
              "1246729   37526413   5914025144  ...  2009-11-21 01:51:01     0\n",
              "1925994   63472113   5759543363  ...  2009-11-16 01:21:12     0\n",
              "536452    38032306  13863317186  ...  2010-05-12 12:46:42     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFMoyenzBbyT"
      },
      "source": [
        "Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhmB5axFBdg4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "92a38f9d-f083-4d5d-eade-65cb708868c1"
      },
      "source": [
        "import re\n",
        "\n",
        "dataset_sample['tweet'] = dataset_sample['tweet'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
        "dataset_sample.dropna(inplace=True)\n",
        "dataset_sample = dataset_sample[dataset_sample.tweet != '']\n",
        "dataset_sample.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4.559301e+06</td>\n",
              "      <td>4.559301e+06</td>\n",
              "      <td>4.559301e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.821544e+07</td>\n",
              "      <td>7.809652e+09</td>\n",
              "      <td>4.714523e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.829647e+07</td>\n",
              "      <td>3.871788e+09</td>\n",
              "      <td>4.991844e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.140000e+02</td>\n",
              "      <td>5.218033e+06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.474197e+07</td>\n",
              "      <td>5.585527e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.941459e+07</td>\n",
              "      <td>5.936591e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.434870e+07</td>\n",
              "      <td>9.118277e+09</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.737670e+08</td>\n",
              "      <td>2.014599e+10</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            user_id      tweet_id         label\n",
              "count  4.559301e+06  4.559301e+06  4.559301e+06\n",
              "mean   5.821544e+07  7.809652e+09  4.714523e-01\n",
              "std    3.829647e+07  3.871788e+09  4.991844e-01\n",
              "min    6.140000e+02  5.218033e+06  0.000000e+00\n",
              "25%    2.474197e+07  5.585527e+09  0.000000e+00\n",
              "50%    4.941459e+07  5.936591e+09  0.000000e+00\n",
              "75%    8.434870e+07  9.118277e+09  1.000000e+00\n",
              "max    1.737670e+08  2.014599e+10  1.000000e+00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT4ext2OC002"
      },
      "source": [
        "Convert to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNQtdaCeC3Xq"
      },
      "source": [
        "# dataset_sample['tweet'] = dataset_sample['tweet'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6KPrNm7E-hv"
      },
      "source": [
        "# Create 2-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9WOPEHB7Kkk"
      },
      "source": [
        "# words = []\n",
        "# for ii in range(0,len(dataset_sample)):\n",
        "#     words.append(str(dataset_sample.iloc[ii]['tweet']).split(\" \"))\n",
        "\n",
        "# n_gram_all = []\n",
        "\n",
        "# for word in words:\n",
        "#     # get n-grams for the instance\n",
        "#     n_gram = []\n",
        "#     for i in range(len(word)-2+1):\n",
        "#         n_gram.append(\"\".join(word[i:i+2]))\n",
        "#     n_gram_all.append(n_gram)\n",
        "    \n",
        "# n_gram_all[0][:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dloLhiHNGGjB"
      },
      "source": [
        "# Vectorizing with Hashing Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnwOLDkqGK6b"
      },
      "source": [
        "# from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
        "\n",
        "# # hash vectorizer instance\n",
        "# #hvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12, alternate_sign= False)\n",
        "\n",
        "# # features matrix X\n",
        "# #X = hvec.fit_transform(n_gram_all)\n",
        "\n",
        "# # alternative\n",
        "# hvec = HashingVectorizer(lowercase=False, n_features=2**13, alternate_sign= False, stop_words='english', norm = None, ngram_range=(1,2))\n",
        "# X = hvec.fit_transform(dataset_sample['tweet'])\n",
        "\n",
        "# print(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJBTXTP5uBP8"
      },
      "source": [
        "## Alternative tokenization using CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjJ3lt-rtj_r"
      },
      "source": [
        "# ## Count Vectorizer \n",
        "\n",
        "# cvec = CountVectorizer(ngram_range=(1,2), stop_words = 'english', max_features=2**12)\n",
        "\n",
        "# # Fit the data and then return the matrix\n",
        "# X = cvec.fit_transform(dataset_sample['tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX7QAR7hAV4u"
      },
      "source": [
        "##TFIDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB_lTs3ZAVTV"
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# tfidf = TfidfVectorizer(ngram_range=(1,1), stop_words='english', max_features=2**12)\n",
        "\n",
        "# X = tfidf.fit_transform(dataset_sample['tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgz9g11AKAbn"
      },
      "source": [
        "# tfidf.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyU1xTZQoiCC"
      },
      "source": [
        "## Split data into training and testing splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAZVpjGvFsoD"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # test set size of 20% of the data and the random seed 1\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X.toarray(), dataset_sample['label'], test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud3sg_qg_2FG"
      },
      "source": [
        "## Multinomial Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jMUXbj6E5BJ"
      },
      "source": [
        "Baseline Implementation - Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRNHhO06-n8F"
      },
      "source": [
        "# from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "# naive_bayes = MultinomialNB()\n",
        "# naive_bayes.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD76SsEMDwtC"
      },
      "source": [
        "# predictions = naive_bayes.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUCos_pI_5l_"
      },
      "source": [
        "# print_metrics(y_test, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YsslltxotH7"
      },
      "source": [
        "Baseline Implementation - 5-fold CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnwovTE-osQ0"
      },
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbV13H7_E6hX"
      },
      "source": [
        "Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gsCI6tTE_NW"
      },
      "source": [
        "# parameters = {'kernel':('linear',), 'C':[1]}\n",
        "# svc = SVC()\n",
        "# clf = GridSearchCV(svc, parameters)\n",
        "# clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPUTxtIh_7Zg"
      },
      "source": [
        "## Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-RClU9REw8i"
      },
      "source": [
        "Baseline Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7LM0C7J_-XS"
      },
      "source": [
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# tree = DecisionTreeClassifier(random_state=17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM8WqiR9Eza4"
      },
      "source": [
        "Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCEB7hkQse6G"
      },
      "source": [
        "# tree_params = {'max_depth': range(4,11),\n",
        "#                'max_features': range(6,19)}\n",
        "\n",
        "# tree_grid = GridSearchCV(tree, tree_params, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# tree_grid.fit(X_train, y_train)\n",
        "\n",
        "# print(tree_grid.best_params_, tree_grid.best_score_)\n",
        "# print_metrics(y_test, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TtUCEd9D_a0"
      },
      "source": [
        "## kNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "733SjuUfD-yI"
      },
      "source": [
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# #knn = KNeighborsClassifier(n_neighbors=5)frac=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y0RS8Ck2rGk"
      },
      "source": [
        "## Finding optimal neighbors with 5-fold CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpvmwOYc2GBV"
      },
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# knn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n",
        "\n",
        "# knn_params = {'knn__n_neighbors': range(1, 2)}\n",
        "\n",
        "# knn_grid = GridSearchCV(knn_pipe, knn_params,\n",
        "#                         cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# knn_grid.fit(X_train, y_train)\n",
        "\n",
        "# knn_grid.best_params_, knn_grid.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrPWrDOX8ClJ"
      },
      "source": [
        "# pred = knn_grid.predict(X_test)\n",
        "# print_metrics(y_test, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76T3MrnuNh9V"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl9t47oRNdCE"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umy_GLH6NoWO"
      },
      "source": [
        "tfd = tf.data.Dataset.from_tensor_slices((dataset_sample[\"tweet\"], dataset_sample[\"label\"]))\n",
        "# tfd.shard()\n",
        "# tfd.shuffle(2*len(tfd)) ##** Why this not work\n",
        "# tfd.shuffle(2*len(tfd))\n",
        "# tfd.shuffle(2*len(tfd))\n",
        "# tfd.shuffle()\n",
        "\n",
        "# tfd = tfd.shuffle(10000)\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "train_dataset = tfd.take(int(len(tfd)*.8)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "#train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = tfd.skip(int(len(tfd)*.8)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "#test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJqCv1dnPEbO"
      },
      "source": [
        "# for example, label in train_dataset.take(1):\n",
        "#   print('text: ', example.numpy())\n",
        "#   print('label: ', label.numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkeqfmBXFDng"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9zjdBoH0avs"
      },
      "source": [
        "# VOCAB_SIZE = 2**10\n",
        "# encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "#     max_tokens=VOCAB_SIZE)\n",
        "# encoder.adapt(tfd.map(lambda text, label: text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nACLJdpx0fBC"
      },
      "source": [
        "# vocab = np.array(encoder.get_vocabulary())\n",
        "# vocab[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVYRiR0_DXbe"
      },
      "source": [
        "# model = tf.keras.Sequential([\n",
        "#     encoder,\n",
        "#     tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "#     tf.keras.layers.Dense(64, activation='relu'),\n",
        "#     tf.keras.layers.Dropout(0.5),\n",
        "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "# ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiSKlOwCJlmN"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# # create a model checkpoint for saving model with highest accuracy\n",
        "filepath = \"model\"\n",
        "\n",
        "earlystopper = EarlyStopping(monitor='val_accuracy', mode='max',patience=10, verbose=1)\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy',verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [earlystopper, checkpoint]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHtF4fBzDleP"
      },
      "source": [
        "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "#               optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "#               metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wl8ltyoDo92"
      },
      "source": [
        "# history = model.fit(train_dataset, epochs=50,\n",
        "#                     validation_data=test_dataset,\n",
        "#                     validation_steps=30, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25fq2KqOkWYm"
      },
      "source": [
        "# tf.keras.models.save_model(model, \"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5k5JLTjNsWa",
        "outputId": "2497763c-7353-4070-ad01-5c09e0a2aad1"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "!pip install tensorflow_text\n",
        "import tensorflow_text as text\n",
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.4.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.36.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub>=0.8.0->tensorflow_text) (56.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.28.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2020.12.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M48V8VZqFRD6",
        "outputId": "d9258133-0fcb-4e20-ac08-de28c4345fae"
      },
      "source": [
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYk5b6bNNix0"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d5i9j2vOL4y",
        "outputId": "fbcfc5af-d056-46df-f43a-44a662ac7a94"
      },
      "source": [
        "text_test = ['this is such an amazing movie!']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v25fQc1gOJo1"
      },
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp92GcmoNoGG",
        "outputId": "d39a1e94-e35b-455c-abd8-280d3291359d"
      },
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.76262873  0.9928097  -0.18611881  0.36673862  0.15233737  0.6550447\n",
            "  0.9681153  -0.9486272   0.00216161 -0.9877732   0.06842697 -0.9763058 ]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[-0.28946346  0.34321272  0.33231515 ...  0.21300808  0.71020764\n",
            "  -0.05771098]\n",
            " [-0.28742087  0.31980985 -0.23018607 ...  0.58455014 -0.21329728\n",
            "   0.72692114]\n",
            " [-0.6615697   0.6887685  -0.87432986 ...  0.10877222 -0.2617324\n",
            "   0.4785539 ]\n",
            " ...\n",
            " [-0.22561133 -0.2892562  -0.07064445 ...  0.47565985  0.83277136\n",
            "   0.40025374]\n",
            " [-0.29824236 -0.27473187 -0.05450562 ...  0.48849723  1.0955356\n",
            "   0.1816333 ]\n",
            " [-0.4437813   0.00930756  0.07223685 ...  0.17290069  1.1833248\n",
            "   0.07897997]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O59DZwlsODf_"
      },
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=\"sigmoid\", name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tCiue-mOYZq",
        "outputId": "d95d34f1-8815-40f2-e016-fb0746aef9d7"
      },
      "source": [
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(tf.sigmoid(bert_raw_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.69455534]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga7Zo7w1OZjb"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = [tf.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J82buS39Oe_T"
      },
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBelHR0ZOhRi"
      },
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "pj4G-IdpOkPV",
        "outputId": "a88515ad-60a1-400a-bed4-941780af7305"
      },
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_dataset,\n",
        "                               validation_data=test_dataset,\n",
        "                               epochs=epochs,\n",
        "                               callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-c49ad7c4b9cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                callbacks=callbacks_list)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[2048,128,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node AdamWeightDecay/gradients/StatefulPartitionedCall/gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_378/gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_378/gradients/bert_encoder/StatefulPartitionedCall_grad/StatefulPartitionedCall/gradients/transformer/layer_0/activation/Gelu/Pow_grad/Pow}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[assert_less_equal_1/Assert/AssertGuard/pivot_f/_976/_327]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[2048,128,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node AdamWeightDecay/gradients/StatefulPartitionedCall/gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_378/gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_378/gradients/bert_encoder/StatefulPartitionedCall_grad/StatefulPartitionedCall/gradients/transformer/layer_0/activation/Gelu/Pow_grad/Pow}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_66490]\n\nFunction call stack:\ntrain_function -> train_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPpLW8j-oRr1"
      },
      "source": [
        "classifier_model.save(\"bert.h5\", include_optimizer=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adWR3II1OlPW"
      },
      "source": [
        "# classifier_model.evaluate(test_dataset, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2Mr38J6Urgo"
      },
      "source": [
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# # ypreds=[]\n",
        "# ypredsbool=np.array([])\n",
        "# ytrue=np.array([])\n",
        "# for i in range(len([item[0] for item in test_dataset.take(-1)])-20):\n",
        "#   y_pred = classifier_model.predict([item[0] for item in test_dataset.take(-1)][i], batch_size=64, verbose=1)\n",
        "#   y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "#   # ypreds=ypreds.append(y_pred)\n",
        "#   ypredsbool = np.concatenate((ypredsbool, y_pred_bool))\n",
        "#   ytrue = np.concatenate((ytrue, [item[1] for item in test_dataset.take()][i]))\n",
        "\n",
        "# print(classification_report(ytrue, ypredsbool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3edX1LGMd0wF"
      },
      "source": [
        "# # from sklearn.metrics import classification_report\n",
        "\n",
        "# y_pred = classifier_model.predict(test_dataset, verbose=1)\n",
        "# y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "# # y_pred_bool\n",
        "\n",
        "# # ypreds=ypreds.append(y_pred)\n",
        "# #   ypredsbool = np.concatenate((ypredsbool, y_pred_bool))\n",
        "# ytrue = [item[1] for item in test_dataset.as_numpy_iterator()]\n",
        "# ytrue = np.array([])\n",
        "# for i in test_dataset.as_numpy_iterator():\n",
        "#   # print(i[1].shape)\n",
        "#   ytrue = np.concatenate((ytrue, i[1]))\n",
        "\n",
        "# ytrue.shape\n",
        "\n",
        "# print(classification_report(ytrue, ypredsbool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jRxd-H8fqqC"
      },
      "source": [
        "len([item[1] for item in test_dataset.as_numpy_iterator()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBoALSnudlj3"
      },
      "source": [
        "print(i[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPgtaJzKchub"
      },
      "source": [
        "np.concatenate((ypredsbool, y_pred_bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCAEtI3jb1jh"
      },
      "source": [
        "# ytrue.shape\n",
        "ypredsbool.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5REzPtaZONF"
      },
      "source": [
        "classifier_model.save(\"bert\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}